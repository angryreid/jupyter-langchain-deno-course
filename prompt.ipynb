{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts\n",
    "\n",
    "How to combine langchain with your prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, John!\n"
     ]
    }
   ],
   "source": [
    "import { PromptTemplate } from '@langchain/core/prompts'\n",
    "\n",
    "const greetings = new PromptTemplate({\n",
    "    inputVariables: ['name'],\n",
    "    template: 'Hello, {name}!'\n",
    "})\n",
    "\n",
    "const formmatedGreetingPrompt = await greetings.format({ name: 'John' })\n",
    "\n",
    "console.log(formmatedGreetingPrompt) // Output: Hello, John!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, John! You are 20 years old.\n"
     ]
    }
   ],
   "source": [
    "const multipleInputVariables = new PromptTemplate({\n",
    "    inputVariables: ['name', 'age'],\n",
    "    template: 'Hello, {name}! You are {age} years old.'\n",
    "})\n",
    "\n",
    "const formattedMultipleInputVariables = await multipleInputVariables.format({ name: 'John', age: 20 })\n",
    "\n",
    "console.log(formattedMultipleInputVariables) // Output: Hello, John! You are 20 years old.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, John! You are 20 years old.\n"
     ]
    }
   ],
   "source": [
    "const autoInferInputVariables = PromptTemplate.fromTemplate('Hello, {name}! You are {age} years old.')\n",
    "\n",
    "const formattedAutoInferInputVariables = await autoInferInputVariables.format({ name: 'John', age: 20 })\n",
    "\n",
    "console.log(formattedAutoInferInputVariables) // Output: Hello, John! You are 20 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, John! You are 30 years old.\n"
     ]
    }
   ],
   "source": [
    "const initialPrompt = new PromptTemplate({\n",
    "    inputVariables: ['name', 'age'],\n",
    "    template: 'Hello, {name}! You are {age} years old.'\n",
    "})\n",
    "\n",
    "const partialPrompt = await initialPrompt.partial({ name: 'John' })\n",
    "\n",
    "const formattedPartialPrompt = await partialPrompt.format({ age: 20 })\n",
    "\n",
    "console.log(formattedPartialPrompt) // Output: Hello, John! You are 20 years old.\n",
    "\n",
    "const formattedPartialPrompt2 = await partialPrompt.format({ age: 30 })\n",
    "\n",
    "console.log(formattedPartialPrompt2) // Output: Hello, John! You are 30 years old.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Templates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, John! The current time is 4:24:40 PM.\n"
     ]
    }
   ],
   "source": [
    "const getCurrentTime = () => new Date().toLocaleTimeString()\n",
    "\n",
    "const dynamicPrompt = new PromptTemplate({\n",
    "    inputVariables: ['name', 'time'],\n",
    "    template: 'Hello, {name}! The current time is {time}.'\n",
    "})\n",
    "\n",
    "const partialPromptWithTime = await dynamicPrompt.partial({\n",
    "    time: getCurrentTime\n",
    "})\n",
    "\n",
    "const formattedDynamicPrompt = await partialPromptWithTime.format({ name: 'John' })\n",
    "\n",
    "console.log(formattedDynamicPrompt) // Output: Hello, John! The current time is 12:00:00 PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Templates\n",
    "\n",
    "### Category\n",
    "\n",
    "1. `ChatPromptTemplate`\n",
    "2. `SystemChatPromptTemplate`\n",
    "3. `AIChatPromptTemplate`\n",
    "4. `HumanChatPromptTemplate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  SystemMessage {\n",
      "    lc_serializable: true,\n",
      "    lc_kwargs: {\n",
      "      content: \"Translate the following from english to japenese.\",\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
      "    content: \"Translate the following from english to japenese.\",\n",
      "    name: undefined,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  HumanMessage {\n",
      "    lc_serializable: true,\n",
      "    lc_kwargs: {\n",
      "      content: \"Plz translate this: Hello world\",\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
      "    content: \"Plz translate this: Hello world\",\n",
      "    name: undefined,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { SystemMessagePromptTemplate } from '@langchain/core/prompts'\n",
    "\n",
    "const translateInstructionTemplate = SystemMessagePromptTemplate.fromTemplate('Translate the following from {source} to {target}.')\n",
    "\n",
    "import { HumanMessagePromptTemplate } from '@langchain/core/prompts'\n",
    "\n",
    "const userQuestionTemplate = HumanMessagePromptTemplate.fromTemplate('Plz translate this: {text}')\n",
    "\n",
    "import { ChatPromptTemplate } from '@langchain/core/prompts'\n",
    "\n",
    "const chatPromptTemplate = ChatPromptTemplate.fromMessages([\n",
    "    translateInstructionTemplate,\n",
    "    userQuestionTemplate\n",
    "])\n",
    "\n",
    "const formattedChatPrompt = await chatPromptTemplate.formatMessages({ source: 'english', target: 'japenese', text: 'Hello world' })\n",
    "\n",
    "console.log(formattedChatPrompt) // Output: Translate the following from en to es. Plz translate this: Hello world\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Testing\n",
    "\n",
    "import { load } from 'dotenv'\n",
    "const env = await load()\n",
    "\n",
    "const process = {\n",
    "    env\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import { ChatOpenAI } from '@langchain/openai'\n",
    "import { StringOutputParser } from '@langchain/core/output_parsers'\n",
    "\n",
    "const chatModel = new ChatOpenAI();\n",
    "const outputParser = new StringOutputParser()\n",
    "\n",
    "const chain = chatModel.pipe(outputParser)\n",
    "\n",
    "await chain.invoke([formattedChatPrompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001b[32m\"Why don't eggs tell jokes? \\n\\nBecause they might crack up!\"\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "  content: \u001b[32m\"Why don't eggs tell jokes? \\n\\nBecause they might crack up!\"\u001b[39m,\n",
       "  name: \u001b[90mundefined\u001b[39m,\n",
       "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "  response_metadata: {\n",
       "    tokenUsage: { completionTokens: \u001b[33m14\u001b[39m, promptTokens: \u001b[33m11\u001b[39m, totalTokens: \u001b[33m25\u001b[39m },\n",
       "    finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const model = new ChatOpenAI();\n",
    "\n",
    "await model.invoke([\n",
    "    new HumanMessage(\"Tell me a joke\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"こんにちわ、世界 (Konnichiwa, sekai)\"\u001b[39m"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatOpenAI } from '@langchain/openai'\n",
    "import { StringOutputParser } from '@langchain/core/output_parsers'\n",
    "\n",
    "const chatModel = new ChatOpenAI();\n",
    "\n",
    "const outputParser = new StringOutputParser()\n",
    "\n",
    "const chain = chatModel.pipe(outputParser)\n",
    "\n",
    "const systemTemplate = 'You are a professional translator. Translate the following from {source} to {target}.'\n",
    "const userTemplate = 'Plz translate this: {text}'\n",
    "\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "    ['system', systemTemplate],\n",
    "    ['human', userTemplate]\n",
    "])\n",
    "\n",
    "const chatChain = chatPrompt.pipe(chatModel).pipe(outputParser)\n",
    "\n",
    "await chatChain.invoke({ source: 'english', target: 'japanese', text: 'Hello world' })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
